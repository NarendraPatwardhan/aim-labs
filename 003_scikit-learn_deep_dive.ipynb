{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scikit-learn: A Deep Dive Tutorial\n\n",
        "This notebook provides a comprehensive walkthrough of `scikit-learn`, the essential library for machine learning in Python. We will cover the entire ML workflow:\n\n",
        "1. **Core Concepts:** The Estimator API (`fit`, `predict`, `transform`).\n",
        "2. **Data Exploration \u0026 Preprocessing:** Loading, visualizing, splitting, and scaling data.\n",
        "3. **Supervised Learning: Classification:** Building and evaluating a classification model visually.\n",
        "4. **Supervised Learning: Regression:** Building and evaluating a regression model visually.\n",
        "5. **Pipelines \u0026 Hyperparameter Tuning:** Best practices for building robust models.\n",
        "6. **Unsupervised Learning:** A brief look at clustering and dimensionality reduction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Standard imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n\n",
        "# Set a nice style for the plots\n",
        "plt.style.use('seaborn-v0_8-whitegrid')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Core Concepts: The Scikit-learn API\n\n",
        "Scikit-learn's power comes from its simple, consistent API. Every algorithm is exposed via an **'Estimator'** object. Key methods include:\n\n",
        "- **`fit(X, y)`**: Trains the model. `X` contains the features (the data), and `y` contains the target labels or values.\n",
        "- **`predict(X_new)`**: Makes predictions on new, unseen data `X_new` after the model has been trained.\n",
        "- **`transform(X)`**: For preprocessing steps, this method transforms the data (e.g., scales it or encodes it).\n",
        "- **`fit_transform(X)`**: A convenience method that combines `fit` and `transform` in one step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Exploration \u0026 Preprocessing\n\n",
        "Before building any model, we must understand and prepare our data. We'll use the famous Iris dataset for our classification example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.datasets import load_iris\n\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Understanding the Dataset\n\n",
        "The Iris dataset contains 150 samples of iris flowers. There are 4 features (sepal length, sepal width, petal length, petal width) and a target variable indicating the species (setosa, versicolor, or virginica). Let's put it into a pandas DataFrame for easier exploration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a pandas DataFrame\n",
        "iris_df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "iris_df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n\n",
        "# Display the first 5 rows\n",
        "iris_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing the Dataset\n\n",
        "A `pairplot` is a fantastic tool to quickly visualize the relationships between all pairs of features, as well as the distribution of each feature, colored by the target variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a pairplot to visualize the data\n",
        "sns.pairplot(iris_df, hue='species', height=2.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train-Test Split\n\n",
        "We must split our data into a training set and a testing set. The model learns from the training set and is evaluated on the unseen testing set to gauge its real-world performance. `stratify=y` ensures that the proportion of each class is the same in both the train and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import train_test_split\n\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n",
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'X_test shape: {X_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Scaling\n\n",
        "Many algorithms perform better when features are on a similar scale. `StandardScaler` standardizes features by removing the mean and scaling to unit variance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.preprocessing import StandardScaler\n\n",
        "scaler = StandardScaler()\n\n",
        "# Fit on the training data ONLY to avoid data leakage\n",
        "scaler.fit(X_train)\n\n",
        "# Transform both train and test data\n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Supervised Learning: Classification\n\n",
        "Let's train a K-Nearest Neighbors (KNN) classifier to predict the species of an iris flower based on its measurements."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n\n",
        "# 1. Instantiate the model\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n\n",
        "# 2. Train the model using the scaled training data\n",
        "knn.fit(X_train_scaled, y_train)\n\n",
        "print('Model trained successfully!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation with Visuals\n\n",
        "While metrics are important, visuals provide deeper insight into model performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\n",
        "y_pred = knn.predict(X_test_scaled)\n",
        "print(f'Model Accuracy: {accuracy_score(y_test, y_pred):.4f}\\n')\n",
        "print('Classification Report:')\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Display the confusion matrix visually\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing the Decision Boundary\n\n",
        "A decision boundary plot shows how the model would classify any point in the feature space. It gives a great intuition for the model's behavior. We will use only the first two features (sepal length and width) for this 2D visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from matplotlib.colors import ListedColormap\n\n",
        "# --- Create a new model trained on only the first two features ---\n",
        "X_train_2d = X_train_scaled[:, :2]\n",
        "X_test_2d = X_test_scaled[:, :2]\n",
        "knn_2d = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_2d.fit(X_train_2d, y_train)\n\n",
        "# --- Create a mesh grid for plotting ---\n",
        "h = .02  # step size in the mesh\n",
        "x_min, x_max = X_train_2d[:, 0].min() - 1, X_train_2d[:, 0].max() + 1\n",
        "y_min, y_max = X_train_2d[:, 1].min() - 1, X_train_2d[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n\n",
        "# --- Make predictions on the mesh grid ---\n",
        "Z = knn_2d.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n\n",
        "# --- Plot the decision boundary and the test points ---\n",
        "plt.figure(figsize=(10, 8))\n",
        "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
        "cmap_bold = ['darkred', 'darkgreen', 'darkblue']\n\n",
        "plt.contourf(xx, yy, Z, cmap=cmap_light)\n\n",
        "sns.scatterplot(x=X_test_2d[:, 0], y=X_test_2d[:, 1], hue=iris.target_names[y_test], palette=cmap_bold, alpha=1.0, edgecolor=\"black\")\n\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())\n",
        "plt.title('2-Class classification (k = 5)')\n",
        "plt.xlabel(iris.feature_names[0])\n",
        "plt.ylabel(iris.feature_names[1])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Supervised Learning: Regression\n\n",
        "Now let's switch to a regression task: predicting a continuous value. We'll use the California Housing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.datasets import fetch_california_housing\n\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "y_pred = lr.predict(X_test_scaled)\n\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n\n",
        "print(f'Mean Squared Error (MSE): {mse:.4f}')\n",
        "print(f'R-squared (R2) Score: {r2:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing Regression Results\n\n",
        "A scatter plot of actual vs. predicted values is a great way to visualize regression performance. For a perfect model, all points would lie on the 45-degree diagonal line."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(y_test, y_pred, alpha=0.3)\n",
        "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], '--r', linewidth=2)\n",
        "plt.xlabel('Actual Values')\n",
        "plt.ylabel('Predicted Values')\n",
        "plt.title('Actual vs. Predicted Values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Pipelines \u0026 Hyperparameter Tuning\n\n",
        "Manually scaling and training can be error-prone. A **Pipeline** chains these steps together into a single estimator object. This prevents data leakage from the test set and simplifies your code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.svm import SVC\n\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svc', SVC(random_state=42))\n",
        "])\n\n",
        "pipeline.fit(X_train, y_train)\n",
        "print(f'Pipeline Accuracy: {pipeline.score(X_test, y_test):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Hyperparameter Tuning with GridSearchCV\n\n",
        "Most models have parameters (hyperparameters) that can be tuned. `GridSearchCV` automates this by exhaustively searching over a specified parameter grid and using cross-validation to find the best combination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import GridSearchCV\n\n",
        "param_grid = {\n",
        "    'svc__C': [0.1, 1, 10, 100],\n",
        "    'svc__gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'svc__kernel': ['rbf', 'linear']\n",
        "}\n\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose=1, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n\n",
        "print(f'\\nBest parameters found: {grid_search.best_params_}')\n",
        "print(f'Best cross-validation score: {grid_search.best_score_:.4f}')\n",
        "print(f'Test set score with best params: {grid_search.score(X_test, y_test):.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Unsupervised Learning\n\n",
        "Unsupervised learning finds patterns in data without pre-existing labels (`y`).\n\n",
        "### Clustering with K-Means\n",
        "K-Means tries to partition data into *k* distinct clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.cluster import KMeans\n\n",
        "X, y = iris.data, iris.target\n\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto')\n",
        "cluster_labels = kmeans.fit_predict(X)\n\n",
        "# Visualize the clusters vs the actual labels\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n\n",
        "ax1.scatter(X[:, 2], X[:, 3], c=cluster_labels, cmap='viridis', s=50)\n",
        "ax1.set_title('K-Means Clusters (on Petal features)')\n",
        "ax1.set_xlabel(iris.feature_names[2])\n",
        "ax1.set_ylabel(iris.feature_names[3])\n\n",
        "ax2.scatter(X[:, 2], X[:, 3], c=y, cmap='viridis', s=50)\n",
        "ax2.set_title('Actual Iris Species')\n",
        "ax2.set_xlabel(iris.feature_names[2])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dimensionality Reduction with PCA\n\n",
        "Principal Component Analysis (PCA) reduces the number of features while preserving as much of the data's variance as possible. It's great for visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.decomposition import PCA\n\n",
        "pca = PCA(n_components=2) # Reduce from 4 features to 2\n",
        "X_pca = pca.fit_transform(X)\n\n",
        "print(f'Original shape: {X.shape}')\n",
        "print(f'Reduced shape: {X_pca.shape}')\n\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=70)\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.title('Iris Dataset Visualized with PCA')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n\n",
        "This notebook has covered the essential workflow of a scikit-learn project, emphasizing visual exploration and evaluation. The library's consistent API for estimators, transformers, and pipelines makes it an incredibly efficient tool for both beginners and experts.\n\n",
        "From here, you can explore the vast number of other algorithms available for classification, regression, clustering, and more, all while using the same fundamental API principles demonstrated here."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}